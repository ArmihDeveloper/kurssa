Rozdział 6: Pamięć i Zapis Danych – Od Scrapera do Reportera
Wprowadzenie
Witaj w szóstej części naszego kursu! Do tej pory zbudowaliśmy niesamowicie zdolnego agenta-scrapera. Potrafi on nawigować po internecie jak człowiek, klikać, wypełniać pola, a nawet radzić sobie z prostymi zabezpieczeniami. Ma jednak jedną, zasadniczą wadę – cierpi na całkowitą amnezję. Po wykonaniu zadania cała ciężko zdobyta wiedza po prostu znika.

W tym rozdziale to zmienimy. Damy naszemu agentowi możliwość tworzenia raportów, zapisywania danych do plików i, co najważniejsze, przekazywania nam tych plików. Przekształcimy go ze scrapera w reportera. Dodatkowo, praktycznie wdrożymy i nauczymy agenta korzystać z serwerów proxy, aby mógł działać jeszcze dyskretniej.

Krok 6.1: Nowe narzędzia w arsenale – Gdzie je znaleźć i jak je dodać?
To kluczowe pytanie, które zadaje sobie każdy początkujący. Odpowiedź jest prosta: tych narzędzi nie "znajdziemy" w żadnej zewnętrznej bibliotece, jak Playwright czy Smol Agents. My je stworzymy od zera jako nowe metody naszej istniejącej klasy SandboxedBrowser.

Pamiętasz, jak mówiliśmy, że ta klasa to nasz "stały pracownik" siedzący przy komputerze w chmurze? Teraz po prostu nauczymy go dwóch nowych sztuczek: zapisywania notatek na dysku i wysyłania nam ich pocztą.

Otwórz swój plik scraper_agent.py i dodaj dwie poniższe metody wewnątrz klasy SandboxedBrowser, zaraz po metodzie solve_and_fill_captcha.

Narzędzie 1: Zapisywanie tekstu do pliku w sandboksie

To narzędzie pozwoli agentowi stworzyć plik o dowolnej nazwie i treści wewnątrz swojego wirtualnego środowiska E2B.

Python

# TĘ METODĘ DODAJESZ WEWNĄTRZ KLASY SandboxedBrowser

async def save_text_to_file(self, filename: str, content: str) -> str:
    """
    Zapisuje podany tekst (content) do pliku o podanej nazwie (filename)
    wewnątrz bezpiecznego środowiska. Przydatne do tworzenia raportów.
    """
    print(f"--- Narzędzie: Zapisuję dane do pliku '{filename}' w sandboksie... ---")
    try:
        # Używamy wbudowanej funkcji sandboksu do zapisu pliku. To takie proste!
        await self.sandbox.filesystem.write(filename, content)
        return f"Pomyślnie zapisano dane w pliku '{filename}' wewnątrz sandboksu."
    except Exception as e:
        return f"Błąd podczas zapisu pliku w sandboksie: {e}"
Narzędzie 2: Pobieranie pliku z sandboksu na nasz komputer

Samo zapisanie pliku w chmurze nam nie wystarczy. Musimy go jakoś stamtąd odebrać. To narzędzie służy jako most między sandboksem a Twoim lokalnym komputerem.

Python

# TĘ METODĘ RÓWNIEŻ DODAJESZ WEWNĄTRZ KLASY SandboxedBrowser

async def download_file_from_sandbox(self, filename: str) -> str:
    """
    Pobiera plik o podanej nazwie (filename) z bezpiecznego środowiska
    i zapisuje go na lokalnym dysku, w folderze, z którego uruchomiono skrypt.
    """
    print(f"--- Narzędzie: Pobieram plik '{filename}' na dysk lokalny... ---")
    try:
        # Najpierw odczytujemy zawartość pliku z sandboksu do pamięci jako bajty
        file_content_bytes = await self.sandbox.filesystem.read_bytes(filename)
        
        # Teraz używamy standardowych funkcji Pythona do zapisu tych bajtów
        # na naszym lokalnym komputerze. 'wb' oznacza 'write bytes' (zapisz bajty).
        with open(filename, "wb") as f:
            f.write(file_content_bytes)
            
        return f"Pomyślnie pobrano plik '{filename}' i zapisano go w bieżącym folderze."
    except Exception as e:
        return f"Błąd podczas pobierania pliku z sandboksu: {e}"
Krok 6.2: Modyfikacja logiki agenta i aktywacja Proxy
Teraz, gdy mamy nowe narzędzia, musimy zaktualizować definicję agenta, aby o nich "wiedział". Damy mu też w końcu możliwość świadomego korzystania z proxy.

Praktyczne użycie Proxy

Zamiast decydować o użyciu proxy na sztywno przy starcie całej sesji, damy agentowi wybór. Zmodyfikujemy w tym celu naszą metodę Maps.

W klasie SandboxedBrowser zastąp starą metodę Maps tą nową wersją:

Python

# TĄ WERSJĄ PODMIENIASZ ISTNIEJĄCĄ METODĘ 'navigate'

async def navigate(self, url: str) -> str:
    """Nawiguje do podanego adresu URL."""
    print(f"--- Narzędzie: Nawiguję do {url} (Proxy: {'Włączone' if self.use_proxy else 'Wyłączone'}) ---")
    await self.page.goto(url, wait_until='domcontentloaded')
    return f"Pomyślnie przeniesiono na stronę: {url}"
A w funkcji main będziemy teraz decydować, czy cała sesja ma korzystać z proxy. Zmienimy use_proxy=False na True, aby to przetestować.

Krok 6.3: Integracja Kodu – Pełna, zaktualizowana wersja scraper_agent.py
Poniżej znajduje się kompletny, zaktualizowany kod Twojego pliku. Zawiera on nowe narzędzia do zapisu i pobierania plików oraz aktywowaną obsługę proxy. Skopiuj go w całości, zastępując poprzednią zawartość pliku.

Python

# PEŁNY, ZAKTUALIZOWANY KOD PLIKU scraper_agent.py

import os
import asyncio
from e2b import Sandbox
from playwright.async_api import async_playwright, Page
from twocaptcha import TwoCaptcha
from smol_agents import Agent
from bs4 import BeautifulSoup

# === KROK 1: Konfiguracja ===
E2B_API_KEY = os.getenv("E2B_API_KEY")
TWOCAPTCHA_API_KEY = os.getenv("TWOCAPTCHA_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Uzupełnij dane swojego proxy. Zostaną użyte, gdy use_proxy=True
PROXY_CONFIG = {
    "server": os.getenv("PROXY_SERVER", "http://twoj_host_proxy:port"),
    "username": os.getenv("PROXY_USERNAME", "twoj_uzytkownik"),
    "password": os.getenv("PROXY_PASSWORD", "twoje_haslo_proxy")
}

# === KROK 2: Klasa Zarządzająca ===
class SandboxedBrowser:
    def __init__(self, use_proxy: bool = False):
        self.sandbox: Sandbox | None = None
        self.page: Page | None = None
        self.use_proxy = use_proxy
        self.captcha_solver = TwoCaptcha(TWOCAPTCHA_API_KEY)

    async def start(self):
        print(f"--- Inicjalizacja: Uruchamiam sandbox (Proxy: {'Włączone' if self.use_proxy else 'Wyłączone'})... ---")
        self.sandbox = await Sandbox.create(template="base", api_key=E2B_API_KEY)
        
        print("--- Inicjalizacja: Instaluję zależności w sandboksie... ---")
        await self.sandbox.process.start_and_wait(
            "apt-get update && apt-get install -y libnss3 libnspr4 libdbus-1-3 libatk1.0-0 libatk-bridge2.0-0 libcups2 libatspi2.0-0 libxcomposite1 libxdamage1 libxfixes3 libxrandr2 libgbm1 libxkbcommon0 libpango-1.0-0 libcairo2 libasound2"
        )
        await self.sandbox.process.start_and_wait("pip install playwright beautifulsoup4 && playwright install chrome")
        
        print("--- Inicjalizacja: Uruchamiam przeglądarkę Chrome w sandboksie... ---")
        browser_args = ['--headless=new']
        if self.use_proxy:
            browser_args.append(f"--proxy-server={PROXY_CONFIG['server']}")
        
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=True, args=browser_args)
        
        context = await self.browser.new_context()
        if self.use_proxy and PROXY_CONFIG['username']:
            await context.set_http_credentials({
                'username': PROXY_CONFIG['username'],
                'password': PROXY_CONFIG['password']
            })

        self.page = await context.new_page()
        print("--- Inicjalizacja: Przeglądarka jest gotowa do pracy! ---")

    async def close(self):
        print("--- Zamykanie: Sprzątam po sobie... ---")
        if hasattr(self, 'browser') and self.browser: await self.browser.close()
        if hasattr(self, 'playwright') and self.playwright: await self.playwright.stop()
        if self.sandbox: await self.sandbox.close()
        print("--- Zamykanie: Sesja zakończona. ---")

    # --- Istniejące i Zmodyfikowane Narzędzia ---
    async def navigate(self, url: str) -> str:
        """Nawiguje do podanego adresu URL."""
        print(f"--- Narzędzie: Nawiguję do {url} (Proxy: {'Włączone' if self.use_proxy else 'Wyłączone'}) ---")
        await self.page.goto(url, wait_until='domcontentloaded')
        return f"Pomyślnie przeniesiono na stronę: {url}"

    async def click_element(self, selector: str) -> str:
        """Klika na element na stronie używając selektora CSS."""
        print(f"--- Narzędzie: Klikam w element '{selector}' ---")
        await self.page.click(selector, timeout=5000)
        return f"Pomyślnie kliknięto w element '{selector}'."

    async def fill_input(self, selector: str, text: str) -> str:
        """Wypełnia pole tekstowe na stronie."""
        print(f"--- Narzędzie: Wypełniam pole '{selector}' tekstem '{text}' ---")
        await self.page.fill(selector, text)
        return f"Pomyślnie wypełniono pole '{selector}'."

    async def get_content(self) -> str:
        """Pobiera i zwraca całą treść tekstową aktualnej strony."""
        print("--- Narzędzie: Pobieram treść strony... ---")
        content_html = await self.page.content()
        await self.sandbox.filesystem.write("/tmp/page.html", content_html)
        script = """
from bs4 import BeautifulSoup
with open('/tmp/page.html', 'r', encoding='utf-8') as f:
    soup = BeautifulSoup(f, 'html.parser')
    for item in soup(["script", "style"]): item.extract()
    print(soup.get_text(separator=' ', strip=True))
"""
        proc = await self.sandbox.process.start_and_wait(f"python -c \"{script}\"")
        return f"Oto treść strony: {proc.stdout}"

    # --- NOWE NARZĘDZIA DODANE W TYM ROZDZIALE ---
    async def save_text_to_file(self, filename: str, content: str) -> str:
        """Zapisuje podany tekst (content) do pliku o podanej nazwie (filename)."""
        print(f"--- Narzędzie: Zapisuję dane do pliku '{filename}' w sandboksie... ---")
        await self.sandbox.filesystem.write(filename, content)
        return f"Pomyślnie zapisano dane w pliku '{filename}'."

    async def download_file_from_sandbox(self, filename: str) -> str:
        """Pobiera plik o podanej nazwie (filename) z sandboksu na dysk lokalny."""
        print(f"--- Narzędzie: Pobieram plik '{filename}' na dysk lokalny... ---")
        file_content_bytes = await self.sandbox.filesystem.read_bytes(filename)
        with open(filename, "wb") as f:
            f.write(file_content_bytes)
        return f"Pomyślnie pobrano plik '{filename}'."

# === KROK 3: Definicja Agenta ===
def create_agent(browser_session: SandboxedBrowser) -> Agent:
    return Agent(
        name="AgentReporter",
        description="Zaawansowany agent do web scrapingu, który potrafi zapisywać wyniki do plików i przekazywać je użytkownikowi. Potrafi też korzystać z proxy.",
        model="gpt-4-turbo",
        tools=[
            # Dodajemy nowe narzędzia do listy, aby agent o nich wiedział!
            browser_session.navigate,
            browser_session.click_element,
            browser_session.fill_input,
            browser_session.get_content,
            browser_session.save_text_to_file,
            browser_session.download_file_from_sandbox,
        ],
        api_key=OPENAI_API_KEY
    )

# === KROK 4: Główny Dyrygent ===
async def main():
    # Zmieniamy use_proxy na True, aby aktywować tę funkcjonalność!
    # Upewnij się, że masz poprawnie skonfigurowane dane w PROXY_CONFIG.
    browser_session = SandboxedBrowser(use_proxy=True) 
    try:
        await browser_session.start()
        agent = create_agent(browser_session)
        
        # Zmieniamy zadanie, aby wykorzystać nowe możliwości
        zadanie = """
        1. Wejdź na stronę 'http://quotes.toscrape.com/'.
        2. Znajdź wszystkie cytaty autorstwa 'Albert Einstein'.
        3. Sformatuj te cytaty jako listę stringów w formacie JSON.
        4. Zapisz wynik do pliku o nazwie 'einstein_quotes.json'.
        5. Pobierz plik 'einstein_quotes.json' na mój komputer.
        """
        print(f"\n--- Zlecenie dla Agenta: {zadanie} ---\n")

        wynik = await agent.arun(zadanie)
        print(f"\n--- Ostateczny Raport Agenta ---\n{wynik}")

    except Exception as e:
        print(f"Wystąpił nieoczekiwany błąd główny: {e}")
    finally:
        await browser_session.close()

if __name__ == "__main__":
    if not all([OPENAI_API_KEY, E2B_API_KEY]):
        print("BŁĄD: Brak klucza OPENAI_API_KEY lub E2B_API_KEY.")
    else:
        asyncio.run(main())

Krok 6.4: Praca ze Strukturami Danych – Jak Rozmawiać z Agentem o JSON i CSV?
Zauważyłeś, jak sformułowaliśmy nowe zadanie?
"Sformatuj te cytaty jako listę stringów w formacie JSON."

To jest właśnie cała magia. Nasze narzędzie save_text_to_file jest bardzo proste – zapisuje dokładnie taki tekst, jaki otrzyma. Ale "mózg" naszego agenta, czyli model GPT-4, doskonale wie, czym jest format JSON.

Kiedy dajesz mu takie polecenie, on sam w swoim procesie "myślowym" wygeneruje poprawnie sformatowany string JSON, a następnie przekaże go jako argument content do naszego narzędzia. My nie musimy pisać żadnej logiki do tworzenia JSON-a!

Gdybyś chciał plik CSV, polecenie mogłoby brzmieć:
"Stwórz plik 'raport.csv' z dwiema kolumnami: 'Cytat' i 'Autor', a następnie wypełnij go danymi."

Agent zrozumie tę instrukcję i przygotuje tekst w odpowiednim formacie. Kluczem jest precyzyjne formułowanie poleceń.

Krok 6.5: Uruchomienie i Weryfikacja Wyników
Upewnij się, że masz wszystko skonfigurowane: aktywne środowisko (venv), ustawione klucze API i poprawne dane w PROXY_CONFIG.
Uruchom skrypt w terminalu:
Bash

python scraper_agent.py
Obserwuj pracę agenta. Zobaczysz w logach, jak korzysta ze znanych Ci już narzędzi, a na końcu wywołuje save_text_to_file i download_file_from_sandbox.
Sprawdź wynik! Po zakończeniu pracy skryptu, zajrzyj do folderu moj-agent-scraper. Powinien się w nim znajdować nowy plik: einstein_quotes.json. Otwórz go i zobacz, czy agent poprawnie wykonał swoje zadanie.
Gratulacje! Twój agent nie jest już tylko scraperem. Potrafi prowadzić badania, tworzyć z nich ustrukturyzowane raporty i dostarczać Ci je bezpośrednio na dysk, a wszystko to robiąc w bardziej anonimowy sposób dzięki proxy. Jesteś gotowy na kolejne, jeszcze bardziej zaawansowane wyzwania!